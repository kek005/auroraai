import torch
import time
import sys
import numpy as np
import scipy
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# âœ… Print System Info
print(f"Torch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Device name: {torch.cuda.get_device_name(0)}")
print(f"NumPy version: {np.__version__}")
print(f"SciPy version: {scipy.__version__}")

# ğŸš€ Ensure GPU is available
if not torch.cuda.is_available():
    sys.exit("âŒ GPU Required! Exiting...")

# ğŸ† Use an optimized model for vLLM
MODEL_NAME = "teknium/OpenHermes-2.5-Mistral-7B"

# ğŸ”¥ Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ğŸš€ Load Model (vLLM keeps it hot in VRAM)
print("ğŸš€ Loading model into VRAM with vLLM...")
llm = LLM(
    model=MODEL_NAME,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.97,  # ğŸ”¥ Use max VRAM
    max_num_seqs=2,  # ğŸ”¥ Reduce batch overhead
    enforce_eager=True,  # ğŸ”¥ Avoid CUDA graph overhead
)

# âœ… Fixed system message for structured responses
system_prompt = (
    "You are an AI interview assistant. Respond concisely and in a structured format. "
    "If a question requires listing multiple items, provide only a bullet-point list, "
    "without explanations or unnecessary details."
)

# âœ… Optimized sampling parameters for fast inference
sampling_params = SamplingParams(
    max_tokens=50,
    temperature=0.1,  # ğŸ”¥ Low = deterministic response
    top_p=0.8,  # ğŸ”¥ Ensures balanced output
    best_of=1,
)

# ğŸ”¥ Warm-up with 50 dummy requests
print("ğŸ”¥ Warming up the model...")
for i in range(50):
    start_time = time.time()
    llm.generate(system_prompt + "\n\nWarm-up question", sampling_params)
    elapsed = time.time() - start_time
    print(f"ğŸ”¥ Warm-up {i+1}/50 | Time: {elapsed:.3f} sec")

print("âœ… Warm-up complete! Ready for real-time interaction.")

# ğŸ¤ Real-Time Input Loop
while True:
    user_input = input("\nğŸ¤– Enter prompt: ")

    if user_input.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Exiting...")
        break

    start_time = time.time()
    full_prompt = system_prompt + "\n\n" + user_input  # ğŸ”¥ NO PREPROCESSING DELAY
    response = llm.generate(full_prompt, sampling_params)
    elapsed = time.time() - start_time

    print(f"\nğŸ’¬ AI Response: {response[0].outputs[0].text.strip()}")
    print(f"âš¡ Response Time: {elapsed:.3f} sec")